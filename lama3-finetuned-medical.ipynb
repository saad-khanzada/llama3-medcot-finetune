{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Environment Setup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nos.environ[\"FLASH_ATTENTION_FORCE_DISABLED\"] = \"1\"\nos.environ[\"DISABLE_TRITON\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:17:52.481881Z","iopub.execute_input":"2025-06-23T18:17:52.482500Z","iopub.status.idle":"2025-06-23T18:17:52.491113Z","shell.execute_reply.started":"2025-06-23T18:17:52.482473Z","shell.execute_reply":"2025-06-23T18:17:52.490329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:17:52.492337Z","iopub.execute_input":"2025-06-23T18:17:52.493050Z","iopub.status.idle":"2025-06-23T18:17:56.241456Z","shell.execute_reply.started":"2025-06-23T18:17:52.493031Z","shell.execute_reply":"2025-06-23T18:17:56.240432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.51.3 trl==0.8.6 bitsandbytes accelerate --no-deps --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:17:56.242586Z","iopub.execute_input":"2025-06-23T18:17:56.242799Z","iopub.status.idle":"2025-06-23T18:17:57.797405Z","shell.execute_reply.started":"2025-06-23T18:17:56.242776Z","shell.execute_reply":"2025-06-23T18:17:57.796295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Verify GPU","metadata":{}},{"cell_type":"code","source":"!nvidia-smi # verify GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:17:57.800208Z","iopub.execute_input":"2025-06-23T18:17:57.800502Z","iopub.status.idle":"2025-06-23T18:17:57.969818Z","shell.execute_reply.started":"2025-06-23T18:17:57.800476Z","shell.execute_reply":"2025-06-23T18:17:57.968878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install Relevent Packages","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:17:57.971048Z","iopub.execute_input":"2025-06-23T18:17:57.971325Z","iopub.status.idle":"2025-06-23T18:18:12.495301Z","shell.execute_reply.started":"2025-06-23T18:17:57.971294Z","shell.execute_reply":"2025-06-23T18:18:12.494414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Dataset Preparation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load dataset from Hugging Face\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n\n# Convert to pandas DataFrame\ndf = pd.DataFrame(dataset[\"train\"])\n\n# Check the column names (optional debug)\nprint(\"Columns:\", df.columns)\nprint(df.head(2))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:12.497095Z","iopub.execute_input":"2025-06-23T18:18:12.497969Z","iopub.status.idle":"2025-06-23T18:18:14.077967Z","shell.execute_reply.started":"2025-06-23T18:18:12.497937Z","shell.execute_reply":"2025-06-23T18:18:14.077025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Combine columns into a formatted prompt-response format","metadata":{}},{"cell_type":"code","source":"# Combine columns into a formatted prompt-response format\ndef format_example(row):\n    return {\n        \"text\": f\"### Question:\\n{row['Question']}\\n\\n### Reasoning:\\n{row['Complex_CoT']}\\n\\n### Answer:\\n{row['Response']}\"\n    }\n\nformatted_data = df.apply(format_example, axis=1)\nformatted_df = pd.DataFrame(formatted_data.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:14.078987Z","iopub.execute_input":"2025-06-23T18:18:14.079675Z","iopub.status.idle":"2025-06-23T18:18:14.390491Z","shell.execute_reply.started":"2025-06-23T18:18:14.079643Z","shell.execute_reply":"2025-06-23T18:18:14.389427Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split dataset","metadata":{}},{"cell_type":"code","source":"# Split dataset\nval_df = formatted_df.sample(n=100, random_state=42)\ntrain_df = formatted_df.drop(val_df.index)\n\n# Convert to Hugging Face datasets format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Display example\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:14.391778Z","iopub.execute_input":"2025-06-23T18:18:14.392270Z","iopub.status.idle":"2025-06-23T18:18:15.057416Z","shell.execute_reply.started":"2025-06-23T18:18:14.392235Z","shell.execute_reply":"2025-06-23T18:18:15.056643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Load LLaMA 3.2 (3B) & Set Fine-Tuning Strategy Using Unsloth","metadata":{}},{"cell_type":"markdown","source":"## 1. Load the Model (4-bit, with LoRA)","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom unsloth import FastLanguageModel\nfrom transformers import AutoTokenizer\n\n# Load Hugging Face token securely from Kaggle secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_Tokens\")\nwandb_token = user_secrets.get_secret(\"wnb\")\n\n\n# Log in to Weights & Biases\nimport wandb\nwandb.login(key=wandb_token)\n\n# Load base model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n    max_seq_length = 2048,\n    dtype = None,     # Let Unsloth choose the best dtype (float16, etc.)\n    load_in_4bit = True,\n    token = hf_token,\n)\n\n# Prepare model for training\nFastLanguageModel.for_training(model,\n    use_gradient_checkpointing = True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:15.058203Z","iopub.execute_input":"2025-06-23T18:18:15.058403Z","iopub.status.idle":"2025-06-23T18:18:27.776507Z","shell.execute_reply.started":"2025-06-23T18:18:15.058388Z","shell.execute_reply":"2025-06-23T18:18:27.775644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n```\n# This is formatted as code\n```\n\n## 2. Prepare the Model for Training with LoRA","metadata":{}},{"cell_type":"code","source":"# Now apply PEFT (LoRA)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,                 # LoRA Rank\n    lora_alpha = 32,        # LoRA Scaling factor\n    lora_dropout = 0.0,    # Dropout\n    bias = \"none\"           # No bias tuning\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:27.779204Z","iopub.execute_input":"2025-06-23T18:18:27.780045Z","iopub.status.idle":"2025-06-23T18:18:31.463074Z","shell.execute_reply.started":"2025-06-23T18:18:27.780024Z","shell.execute_reply":"2025-06-23T18:18:31.462385Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Tokenize the Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize(example):\n    tokenized = tokenizer(\n        example[\"text\"],\n        truncation = True,\n        padding = \"max_length\",\n        max_length = 2048\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:31.463951Z","iopub.execute_input":"2025-06-23T18:18:31.464262Z","iopub.status.idle":"2025-06-23T18:18:31.468994Z","shell.execute_reply.started":"2025-06-23T18:18:31.464239Z","shell.execute_reply":"2025-06-23T18:18:31.468118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.map(tokenize)\nval_dataset = val_dataset.map(tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:18:31.469919Z","iopub.execute_input":"2025-06-23T18:18:31.470466Z","iopub.status.idle":"2025-06-23T18:19:36.824297Z","shell.execute_reply.started":"2025-06-23T18:18:31.470448Z","shell.execute_reply":"2025-06-23T18:19:36.823458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Set Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = \"llama3-medical-finetuning\",  # Where the model checkpoints will be saved\n    per_device_train_batch_size = 2,  # Effective batch size = 2 * gradient_accumulation_steps\n    gradient_accumulation_steps = 2,  # Accumulates gradients for more stable training\n    max_steps = 60,  # Small number for quick test run\n    logging_steps = 1,  # Logs every step for debugging\n    save_steps = 10,  # Saves model every 10 steps\n    learning_rate = 2e-4,  # A good starting point for PEFT\n    num_train_epochs = 1,  # Will be overridden if max_steps is reached first\n    fp16 = True,  # You can turn this ON if you want mixed-precision on Colab Pro/Pro+ GPUs\n    optim = \"adamw_torch\",  # Preferable over \"paged_adamw_32bit\" if that caused issues\n    lr_scheduler_type = \"cosine\",  # Smooth learning rate curve\n    warmup_steps = 5,  # Start with low LR for stability\n    report_to = \"wandb\",  # Disable W&B\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.825360Z","iopub.execute_input":"2025-06-23T18:19:36.825678Z","iopub.status.idle":"2025-06-23T18:19:36.855025Z","shell.execute_reply.started":"2025-06-23T18:19:36.825650Z","shell.execute_reply":"2025-06-23T18:19:36.854411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. formatting_func for Your Dataset","metadata":{}},{"cell_type":"code","source":"print(val_df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.855850Z","iopub.execute_input":"2025-06-23T18:19:36.856166Z","iopub.status.idle":"2025-06-23T18:19:36.860738Z","shell.execute_reply.started":"2025-06-23T18:19:36.856142Z","shell.execute_reply":"2025-06-23T18:19:36.859962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(val_df[\"text\"].iloc[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.861597Z","iopub.execute_input":"2025-06-23T18:19:36.862130Z","iopub.status.idle":"2025-06-23T18:19:36.875903Z","shell.execute_reply.started":"2025-06-23T18:19:36.862104Z","shell.execute_reply":"2025-06-23T18:19:36.874960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract Question, Reasoning, and Answer with Regex","metadata":{}},{"cell_type":"code","source":"import re\n\ndef extract_fields(text):\n    question_match = re.search(r\"### Question:\\n(.+?)\\n### Reasoning:\", text, re.DOTALL)\n    reasoning_match = re.search(r\"### Reasoning:\\n(.+?)\\n### Answer:\", text, re.DOTALL)\n    answer_match = re.search(r\"### Answer:\\n(.+)\", text, re.DOTALL)\n\n    return {\n        \"Question\": question_match.group(1).strip() if question_match else None,\n        \"Complex_CoT\": reasoning_match.group(1).strip() if reasoning_match else None,\n        \"Response\": answer_match.group(1).strip() if answer_match else None,\n    }\n\n# Apply to all rows\nparsed_df = val_df[\"text\"].apply(extract_fields).apply(pd.Series)\n\n# Merge with original dataframe if needed\nval_df = pd.concat([val_df, parsed_df], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.876738Z","iopub.execute_input":"2025-06-23T18:19:36.877019Z","iopub.status.idle":"2025-06-23T18:19:36.913318Z","shell.execute_reply.started":"2025-06-23T18:19:36.876992Z","shell.execute_reply":"2025-06-23T18:19:36.912282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def formatting_func(example):\n    question = example[\"Question\"]\n    reasoning = example[\"Complex_CoT\"]\n    response = example[\"Response\"]\n\n    return f\"### Question:\\n{question}\\n\\n### Reasoning:\\n{reasoning}\\n\\n### Answer:\\n{response}\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.914208Z","iopub.execute_input":"2025-06-23T18:19:36.914986Z","iopub.status.idle":"2025-06-23T18:19:36.919155Z","shell.execute_reply.started":"2025-06-23T18:19:36.914959Z","shell.execute_reply":"2025-06-23T18:19:36.918548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SFTTrainer Setup","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    args=training_args,\n    tokenizer=tokenizer,\n    formatting_func=formatting_func,\n    packing=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.919843Z","iopub.execute_input":"2025-06-23T18:19:36.920123Z","iopub.status.idle":"2025-06-23T18:19:36.949768Z","shell.execute_reply.started":"2025-06-23T18:19:36.920107Z","shell.execute_reply":"2025-06-23T18:19:36.949202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ROUGE-L Score Calculation (Before Training (Baseline Score))","metadata":{}},{"cell_type":"code","source":"print(val_df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.951068Z","iopub.execute_input":"2025-06-23T18:19:36.951316Z","iopub.status.idle":"2025-06-23T18:19:36.955335Z","shell.execute_reply.started":"2025-06-23T18:19:36.951293Z","shell.execute_reply":"2025-06-23T18:19:36.954513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required packages\n!pip install -q evaluate rouge_score\n\nimport evaluate\nrouge = evaluate.load(\"rouge\")\n\n# Get baseline predictions\ndef generate_response_baseline(example):\n    prompt = formatting_func(example)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=200)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Apply to validation set\nval_df[\"baseline_pred\"] = val_df.apply(generate_response_baseline, axis=1)\n\n# Compute ROUGE-L score\nbaseline_scores = rouge.compute(predictions=val_df[\"baseline_pred\"].tolist(),\n                                 references=val_df[\"Response\"].tolist(),\n                                 use_stemmer=True)\nprint(\"ROUGE-L Before Fine-Tuning:\", baseline_scores[\"rougeL\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:19:36.956135Z","iopub.execute_input":"2025-06-23T18:19:36.956373Z","iopub.status.idle":"2025-06-23T18:22:35.220305Z","shell.execute_reply.started":"2025-06-23T18:19:36.956348Z","shell.execute_reply":"2025-06-23T18:22:35.219290Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training\n\nThis will:\n\n    Start supervised fine-tuning on medical dataset.\n\n    Log metrics (e.g., loss) to the console and to Weights & Biases (since we're using report_to=\"wandb\").","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(settings=wandb.Settings(init_timeout=120))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:22:35.221446Z","iopub.execute_input":"2025-06-23T18:22:35.221725Z","iopub.status.idle":"2025-06-23T18:22:41.366702Z","shell.execute_reply.started":"2025-06-23T18:22:35.221698Z","shell.execute_reply":"2025-06-23T18:22:41.365880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:22:41.367589Z","iopub.execute_input":"2025-06-23T18:22:41.367778Z","iopub.status.idle":"2025-06-23T18:47:23.223636Z","shell.execute_reply.started":"2025-06-23T18:22:41.367763Z","shell.execute_reply":"2025-06-23T18:47:23.222661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## After Training (Post Fine-Tuning Score)","metadata":{}},{"cell_type":"code","source":"# Reload fine-tuned model (if necessary) and run predictions again\ndef generate_response_finetuned(example):\n    prompt = formatting_func(example)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=200)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nval_df[\"finetuned_pred\"] = val_df.apply(generate_response_finetuned, axis=1)\nfinetuned_scores = rouge.compute(predictions=val_df[\"finetuned_pred\"].tolist(),\n                                  references=val_df[\"Response\"].tolist(),\n                                  use_stemmer=True)\nprint(\"ROUGE-L After Fine-Tuning:\", finetuned_scores[\"rougeL\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:47:23.225009Z","iopub.execute_input":"2025-06-23T18:47:23.225686Z","iopub.status.idle":"2025-06-23T18:56:41.581749Z","shell.execute_reply.started":"2025-06-23T18:47:23.225666Z","shell.execute_reply":"2025-06-23T18:56:41.580610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## save the model","metadata":{}},{"cell_type":"code","source":"save_path = \"llama3-medical-finetuned\"\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:56:41.582777Z","iopub.execute_input":"2025-06-23T18:56:41.583228Z","iopub.status.idle":"2025-06-23T18:56:42.257452Z","shell.execute_reply.started":"2025-06-23T18:56:41.583199Z","shell.execute_reply":"2025-06-23T18:56:42.256580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:56:42.258456Z","iopub.execute_input":"2025-06-23T18:56:42.259125Z","iopub.status.idle":"2025-06-23T18:56:42.276617Z","shell.execute_reply.started":"2025-06-23T18:56:42.259095Z","shell.execute_reply":"2025-06-23T18:56:42.275662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.push_to_hub(\"SaadKabeer/llama3-medical-finetuned\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T19:02:54.345306Z","iopub.execute_input":"2025-06-23T19:02:54.345948Z","iopub.status.idle":"2025-06-23T19:02:58.241939Z","shell.execute_reply.started":"2025-06-23T19:02:54.345904Z","shell.execute_reply":"2025-06-23T19:02:58.241048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.push_to_hub(\"SaadKabeer/llama3-medical-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T19:03:05.645027Z","iopub.execute_input":"2025-06-23T19:03:05.645847Z","iopub.status.idle":"2025-06-23T19:03:07.522727Z","shell.execute_reply.started":"2025-06-23T19:03:05.645819Z","shell.execute_reply":"2025-06-23T19:03:07.521836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}