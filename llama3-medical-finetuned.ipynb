{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Environment Setup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nos.environ[\"FLASH_ATTENTION_FORCE_DISABLED\"] = \"1\"\nos.environ[\"DISABLE_TRITON\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:48.140506Z","iopub.execute_input":"2025-07-02T14:24:48.141068Z","iopub.status.idle":"2025-07-02T14:24:48.144754Z","shell.execute_reply.started":"2025-07-02T14:24:48.141044Z","shell.execute_reply":"2025-07-02T14:24:48.143930Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:48.145759Z","iopub.execute_input":"2025-07-02T14:24:48.146086Z","iopub.status.idle":"2025-07-02T14:24:48.193550Z","shell.execute_reply.started":"2025-07-02T14:24:48.146062Z","shell.execute_reply":"2025-07-02T14:24:48.192684Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:48.194374Z","iopub.execute_input":"2025-07-02T14:24:48.194636Z","iopub.status.idle":"2025-07-02T14:24:51.763736Z","shell.execute_reply.started":"2025-07-02T14:24:48.194617Z","shell.execute_reply":"2025-07-02T14:24:51.762891Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!pip install transformers==4.51.3 trl==0.8.6 bitsandbytes accelerate --no-deps --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:51.765826Z","iopub.execute_input":"2025-07-02T14:24:51.766167Z","iopub.status.idle":"2025-07-02T14:24:53.235846Z","shell.execute_reply.started":"2025-07-02T14:24:51.766140Z","shell.execute_reply":"2025-07-02T14:24:53.235050Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Verify GPU","metadata":{}},{"cell_type":"code","source":"!nvidia-smi # verify GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:53.236922Z","iopub.execute_input":"2025-07-02T14:24:53.237242Z","iopub.status.idle":"2025-07-02T14:24:53.453020Z","shell.execute_reply.started":"2025-07-02T14:24:53.237206Z","shell.execute_reply":"2025-07-02T14:24:53.452047Z"}},"outputs":[{"name":"stdout","text":"Wed Jul  2 14:24:53 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             32W /  250W |     257MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Install Relevent Packages","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:53.454166Z","iopub.execute_input":"2025-07-02T14:24:53.454424Z","iopub.status.idle":"2025-07-02T14:24:53.459697Z","shell.execute_reply.started":"2025-07-02T14:24:53.454400Z","shell.execute_reply":"2025-07-02T14:24:53.458862Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Step 2: Dataset Preparation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load dataset from Hugging Face\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n\n# Convert to pandas DataFrame\ndf = pd.DataFrame(dataset[\"train\"])\n\n# Check the column names (optional debug)\nprint(\"Columns:\", df.columns)\nprint(df.head(2))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:53.460596Z","iopub.execute_input":"2025-07-02T14:24:53.460844Z","iopub.status.idle":"2025-07-02T14:24:55.422602Z","shell.execute_reply.started":"2025-07-02T14:24:53.460823Z","shell.execute_reply":"2025-07-02T14:24:55.421802Z"}},"outputs":[{"name":"stdout","text":"Columns: Index(['Question', 'Complex_CoT', 'Response'], dtype='object')\n                                            Question  \\\n0  Given the symptoms of sudden weakness in the l...   \n1  A 33-year-old woman is brought to the emergenc...   \n\n                                         Complex_CoT  \\\n0  Okay, let's see what's going on here. We've go...   \n1  Okay, let's figure out what's going on here. A...   \n\n                                            Response  \n0  The specific cardiac abnormality most likely t...  \n1  In this scenario, the most likely anatomical s...  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Combine columns into a formatted prompt-response format","metadata":{}},{"cell_type":"code","source":"# Combine columns into a formatted prompt-response format\ndef format_example(row):\n    return {\n        \"text\": f\"### Question:\\n{row['Question']}\\n\\n### Reasoning:\\n{row['Complex_CoT']}\\n\\n### Answer:\\n{row['Response']}\"\n    }\n\nformatted_data = df.apply(format_example, axis=1)\nformatted_df = pd.DataFrame(formatted_data.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:55.423500Z","iopub.execute_input":"2025-07-02T14:24:55.423956Z","iopub.status.idle":"2025-07-02T14:24:55.632867Z","shell.execute_reply.started":"2025-07-02T14:24:55.423927Z","shell.execute_reply":"2025-07-02T14:24:55.632201Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Split dataset","metadata":{}},{"cell_type":"code","source":"# Split dataset\nval_df = formatted_df.sample(n=100, random_state=42)\ntrain_df = formatted_df.drop(val_df.index)\n\n# Convert to Hugging Face datasets format\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Display example\nprint(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:55.635765Z","iopub.execute_input":"2025-07-02T14:24:55.636003Z","iopub.status.idle":"2025-07-02T14:24:56.229452Z","shell.execute_reply.started":"2025-07-02T14:24:55.635985Z","shell.execute_reply":"2025-07-02T14:24:56.228712Z"}},"outputs":[{"name":"stdout","text":"{'text': \"### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Reasoning:\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n\\n### Answer:\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.\", '__index_level_0__': 0}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Step 3: Load LLaMA 3.2 (3B) & Set Fine-Tuning Strategy Using Unsloth","metadata":{}},{"cell_type":"markdown","source":"## 1. Load the Model (4-bit, with LoRA)","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom unsloth import FastLanguageModel\nfrom transformers import AutoTokenizer\n\n# Load Hugging Face token securely from Kaggle secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_Tokens\")\nwandb_token = user_secrets.get_secret(\"wnb\")\n\n\n# Log in to Weights & Biases\nimport wandb\nwandb.login(key=wandb_token)\n\n# Load base model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n    max_seq_length = 2048,\n    dtype = None,     # Let Unsloth choose the best dtype (float16, etc.)\n    load_in_4bit = True,\n    token = hf_token,\n)\n\n# Prepare model for training\nFastLanguageModel.for_training(model,\n    use_gradient_checkpointing = True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:24:56.230137Z","iopub.execute_input":"2025-07-02T14:24:56.230398Z","iopub.status.idle":"2025-07-02T14:25:25.367212Z","shell.execute_reply.started":"2025-07-02T14:24:56.230375Z","shell.execute_reply":"2025-07-02T14:25:25.366515Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaadkhanzada54\u001b[0m (\u001b[33msaad_khanzada\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2025.6.12: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd30b98fb6c48508274c6a1b5c569ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50deaa70d30e4ccf90bd7db8791c4bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c57c8021582451288c47eae540632c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"888071c44005413eb0714c951825d6ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"defc65cc96f54523874084849a248e20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dadca094f3240779dbf5be28237c9e4"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n    (layers): ModuleList(\n      (0): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n      (1): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n      (2-27): 26 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"\n\n```\n# This is formatted as code\n```\n\n## 2. Prepare the Model for Training with LoRA","metadata":{}},{"cell_type":"code","source":"# Now apply PEFT (LoRA)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,                 # LoRA Rank\n    lora_alpha = 32,        # LoRA Scaling factor\n    lora_dropout = 0.0,    # Dropout\n    bias = \"none\"           # No bias tuning\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:25:25.368089Z","iopub.execute_input":"2025-07-02T14:25:25.368724Z","iopub.status.idle":"2025-07-02T14:25:29.250343Z","shell.execute_reply.started":"2025-07-02T14:25:25.368706Z","shell.execute_reply":"2025-07-02T14:25:29.249336Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.6.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 3. Tokenize the Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize(example):\n    tokenized = tokenizer(\n        example[\"text\"],\n        truncation = True,\n        padding = \"max_length\",\n        max_length = 2048\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:25:29.251270Z","iopub.execute_input":"2025-07-02T14:25:29.251524Z","iopub.status.idle":"2025-07-02T14:25:29.780845Z","shell.execute_reply.started":"2025-07-02T14:25:29.251505Z","shell.execute_reply":"2025-07-02T14:25:29.780033Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_dataset = train_dataset.map(tokenize)\nval_dataset = val_dataset.map(tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:25:29.781686Z","iopub.execute_input":"2025-07-02T14:25:29.781969Z","iopub.status.idle":"2025-07-02T14:26:30.253143Z","shell.execute_reply.started":"2025-07-02T14:25:29.781941Z","shell.execute_reply":"2025-07-02T14:26:30.252125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19604 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45e3ab496c9247ffaa435f2d01221880"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fa86b57fc4488b91d217c40ffba3bb"}},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## 4. Set Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = \"llama3-medical-finetuning\",  # Where the model checkpoints will be saved\n    per_device_train_batch_size = 2,  # Effective batch size = 2 * gradient_accumulation_steps\n    gradient_accumulation_steps = 2,  # Accumulates gradients for more stable training\n    max_steps = 60,  # Small number for quick test run\n    logging_steps = 1,  # Logs every step for debugging\n    save_steps = 10,  # Saves model every 10 steps\n    learning_rate = 2e-4,  # A good starting point for PEFT\n    num_train_epochs = 1,  # Will be overridden if max_steps is reached first\n    fp16 = True,  # You can turn this ON if you want mixed-precision on Colab Pro/Pro+ GPUs\n    optim = \"adamw_torch\",  # Preferable over \"paged_adamw_32bit\" if that caused issues\n    lr_scheduler_type = \"cosine\",  # Smooth learning rate curve\n    warmup_steps = 5,  # Start with low LR for stability\n    report_to = \"wandb\",  # Disable W&B\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.254291Z","iopub.execute_input":"2025-07-02T14:26:30.254593Z","iopub.status.idle":"2025-07-02T14:26:30.284666Z","shell.execute_reply.started":"2025-07-02T14:26:30.254570Z","shell.execute_reply":"2025-07-02T14:26:30.284018Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## 5. formatting_func for Your Dataset","metadata":{}},{"cell_type":"code","source":"print(val_df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.285476Z","iopub.execute_input":"2025-07-02T14:26:30.285811Z","iopub.status.idle":"2025-07-02T14:26:30.290077Z","shell.execute_reply.started":"2025-07-02T14:26:30.285787Z","shell.execute_reply":"2025-07-02T14:26:30.289298Z"}},"outputs":[{"name":"stdout","text":"['text']\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(val_df[\"text\"].iloc[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.290984Z","iopub.execute_input":"2025-07-02T14:26:30.291179Z","iopub.status.idle":"2025-07-02T14:26:30.308965Z","shell.execute_reply.started":"2025-07-02T14:26:30.291164Z","shell.execute_reply":"2025-07-02T14:26:30.308192Z"}},"outputs":[{"name":"stdout","text":"### Question:\nA 24-year-old woman has progressively worsening episodes of severe, crampy abdominal pain, nonbloody diarrhea, mild abdominal distension, and a perianal fistula draining stool. Immunohistochemistry reveals dysfunction of the nucleotide oligomerization binding domain 2 (NOD2) protein. Which immunological protein is most likely overactive due to this dysfunction?\n\n### Reasoning:\nHmm, a 24-year-old woman is experiencing these terrible stomach issues, like severe cramps and diarrhea, plus she's got a perianal fistula that's really uncomfortable. It sounds like a tough situation.\n\nFrom what I know, these symptoms kinda fit the description of Crohn's disease, doesn't it? Crohn's is one of those inflammatory bowel diseases where the immune system kinda goes haywire.\n\nNow, let's think about this NOD2 thing she's got going on. NOD2 is an important protein that helps recognize bacteria, like a bodyguard for our gut. If dysfunctional, it can lead to problems with detecting bacteria the right way and seems it's linked with Crohn's disease.\n\nOh, right, NOD2 issues often lead to changes in the NF-kB pathway. NF-kB is this transcription factor, a big player in the immune system that helps regulate inflammation.\n\nIn cases where NOD2 isn't working like it should, I remember NF-kB can go into overdrive â€” kinda like a car with a stuck accelerator, causing inflammation.\n\nAnd thinking about her symptoms again, this chronic inflammation might explain her abdominal pain and that fistula. Theyâ€™re classic signs of inadequate regulation and excessive inflammation typical in Crohnâ€™s.\n\nAlso, with these NOD2 defects making NF-kB more active than it should be, I can see how Crohnâ€™s emerges with such symptoms.\n\nOkay, so putting it all together, it seems this overactivity of NF-kB, driven by the NOD2 dysfunction, is probably causing her problems. It fits, right? Yeah, I think so.\n\n### Answer:\nThe immunological protein that is most likely overactive due to the dysfunction of the NOD2 protein is NF-kB (nuclear factor kappa-light-chain-enhancer of activated B cells). NOD2 plays a crucial role in regulating the immune response to bacterial antigens in the gut. When NOD2 is dysfunctional, it can result in the inappropriate activation of the NF-kB pathway, leading to excessive inflammation. This overactivity of NF-kB is characteristic of conditions like Crohn's disease, which matches the symptoms exhibited by the woman, such as severe abdominal pain, diarrhea, and fistula formation.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Extract Question, Reasoning, and Answer with Regex","metadata":{}},{"cell_type":"code","source":"import re\n\ndef extract_fields(text):\n    question_match = re.search(r\"### Question:\\n(.+?)\\n### Reasoning:\", text, re.DOTALL)\n    reasoning_match = re.search(r\"### Reasoning:\\n(.+?)\\n### Answer:\", text, re.DOTALL)\n    answer_match = re.search(r\"### Answer:\\n(.+)\", text, re.DOTALL)\n\n    return {\n        \"Question\": question_match.group(1).strip() if question_match else None,\n        \"Complex_CoT\": reasoning_match.group(1).strip() if reasoning_match else None,\n        \"Response\": answer_match.group(1).strip() if answer_match else None,\n    }\n\n# Apply to all rows\nparsed_df = val_df[\"text\"].apply(extract_fields).apply(pd.Series)\n\n# Merge with original dataframe if needed\nval_df = pd.concat([val_df, parsed_df], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.309694Z","iopub.execute_input":"2025-07-02T14:26:30.310050Z","iopub.status.idle":"2025-07-02T14:26:30.354976Z","shell.execute_reply.started":"2025-07-02T14:26:30.310028Z","shell.execute_reply":"2025-07-02T14:26:30.354185Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def formatting_func(example):\n    question = example[\"Question\"]\n    reasoning = example[\"Complex_CoT\"]\n    response = example[\"Response\"]\n\n    return f\"### Question:\\n{question}\\n\\n### Reasoning:\\n{reasoning}\\n\\n### Answer:\\n{response}\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.355776Z","iopub.execute_input":"2025-07-02T14:26:30.356058Z","iopub.status.idle":"2025-07-02T14:26:30.360791Z","shell.execute_reply.started":"2025-07-02T14:26:30.356033Z","shell.execute_reply":"2025-07-02T14:26:30.360083Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## SFTTrainer Setup","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    args=training_args,\n    tokenizer=tokenizer,\n    formatting_func=formatting_func,\n    packing=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.361587Z","iopub.execute_input":"2025-07-02T14:26:30.361762Z","iopub.status.idle":"2025-07-02T14:26:30.392546Z","shell.execute_reply.started":"2025-07-02T14:26:30.361749Z","shell.execute_reply":"2025-07-02T14:26:30.391835Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## ROUGE-L Score Calculation (Before Training (Baseline Score))","metadata":{}},{"cell_type":"code","source":"print(val_df.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.393238Z","iopub.execute_input":"2025-07-02T14:26:30.393477Z","iopub.status.idle":"2025-07-02T14:26:30.398110Z","shell.execute_reply.started":"2025-07-02T14:26:30.393461Z","shell.execute_reply":"2025-07-02T14:26:30.397337Z"}},"outputs":[{"name":"stdout","text":"['text', 'Question', 'Complex_CoT', 'Response']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Install required packages\n!pip install -q evaluate rouge_score\n\nimport evaluate\nrouge = evaluate.load(\"rouge\")\n\n# Get baseline predictions\ndef generate_response_baseline(example):\n    prompt = formatting_func(example)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=200)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Apply to validation set\nval_df[\"baseline_pred\"] = val_df.apply(generate_response_baseline, axis=1)\n\n# Compute ROUGE-L score\nbaseline_scores = rouge.compute(predictions=val_df[\"baseline_pred\"].tolist(),\n                                 references=val_df[\"Response\"].tolist(),\n                                 use_stemmer=True)\nprint(\"ROUGE-L Before Fine-Tuning:\", baseline_scores[\"rougeL\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:26:30.399279Z","iopub.execute_input":"2025-07-02T14:26:30.399515Z","iopub.status.idle":"2025-07-02T14:29:24.883506Z","shell.execute_reply.started":"2025-07-02T14:26:30.399499Z","shell.execute_reply":"2025-07-02T14:29:24.882658Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32320250df44440088bfc52fc9477621"}},"metadata":{}},{"name":"stdout","text":"ROUGE-L Before Fine-Tuning: 0.3054163938869806\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Training\n\nThis will:\n\n    Start supervised fine-tuning on medical dataset.\n\n    Log metrics (e.g., loss) to the console and to Weights & Biases (since we're using report_to=\"wandb\").","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.init(settings=wandb.Settings(init_timeout=120))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:29:24.884411Z","iopub.execute_input":"2025-07-02T14:29:24.884673Z","iopub.status.idle":"2025-07-02T14:29:31.286088Z","shell.execute_reply.started":"2025-07-02T14:29:24.884650Z","shell.execute_reply":"2025-07-02T14:29:31.285394Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250702_142924-r8ea0mhp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/saad_khanzada/uncategorized/runs/r8ea0mhp' target=\"_blank\">glamorous-wave-13</a></strong> to <a href='https://wandb.ai/saad_khanzada/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/saad_khanzada/uncategorized' target=\"_blank\">https://wandb.ai/saad_khanzada/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/saad_khanzada/uncategorized/runs/r8ea0mhp' target=\"_blank\">https://wandb.ai/saad_khanzada/uncategorized/runs/r8ea0mhp</a>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/saad_khanzada/uncategorized/runs/r8ea0mhp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7b5694300e90>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:29:31.286839Z","iopub.execute_input":"2025-07-02T14:29:31.287199Z","iopub.status.idle":"2025-07-02T14:54:16.939761Z","shell.execute_reply.started":"2025-07-02T14:29:31.287174Z","shell.execute_reply":"2025-07-02T14:54:16.938989Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 19,604 | Num Epochs = 1 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n \"-____-\"     Trainable parameters = 24,313,856 of 3,000,000,000 (0.81% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 24:17, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.549600</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>10.559400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>9.905600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>9.607500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>7.662800</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>7.618400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>6.543100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>6.044400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>5.657000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>4.758600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>5.029800</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>5.120300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>4.830200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>4.778300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>4.856100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>4.768100</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>4.939400</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>4.635100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>4.407800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>4.821200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>4.774900</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>4.488600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.733100</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>4.818800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>4.681200</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>4.810800</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>4.776500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>4.627200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>4.584000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>4.631400</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>4.586500</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>4.504700</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>4.754600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>4.604900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>4.420100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>4.616400</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>4.787900</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>4.649700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.549900</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>4.751300</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>4.543700</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>4.296500</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>4.674400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>4.630300</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>4.574100</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>4.784000</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>4.555700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>4.537200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>4.674900</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>4.596100</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>4.735200</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>4.506300</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>4.611800</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>4.414800</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>4.427000</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>4.325100</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>4.691500</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>4.710800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.663900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=60, training_loss=5.236639833450317, metrics={'train_runtime': 1483.1732, 'train_samples_per_second': 0.162, 'train_steps_per_second': 0.04, 'total_flos': 8384528787701760.0, 'train_loss': 5.236639833450317, 'epoch': 0.012242399510304019})"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"## After Training (Post Fine-Tuning Score)","metadata":{}},{"cell_type":"code","source":"# Reload fine-tuned model (if necessary) and run predictions again\ndef generate_response_finetuned(example):\n    prompt = formatting_func(example)\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=200)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nval_df[\"finetuned_pred\"] = val_df.apply(generate_response_finetuned, axis=1)\nfinetuned_scores = rouge.compute(predictions=val_df[\"finetuned_pred\"].tolist(),\n                                  references=val_df[\"Response\"].tolist(),\n                                  use_stemmer=True)\nprint(\"ROUGE-L After Fine-Tuning:\", finetuned_scores[\"rougeL\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:02:57.958505Z","iopub.execute_input":"2025-07-02T15:02:57.958787Z","iopub.status.idle":"2025-07-02T15:09:18.078143Z","shell.execute_reply.started":"2025-07-02T15:02:57.958765Z","shell.execute_reply":"2025-07-02T15:09:18.077349Z"}},"outputs":[{"name":"stdout","text":"ROUGE-L After Fine-Tuning: 0.3055316456891676\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"### ðŸ“Š BERTScore Evaluation â€“ Semantic Quality Assessment\n\nTo assess the semantic similarity between the **model-generated responses** and the **ground-truth answers**, we use **BERTScore**, a state-of-the-art metric that captures deep contextual similarity rather than relying on mere word overlap.\n\n> âœ… **Why BERTScore?**  \n> Unlike traditional metrics (e.g., ROUGE, BLEU), BERTScore uses pre-trained transformer embeddings (such as **DeBERTa v3 Base**) to measure how *semantically* similar two texts are â€” making it ideal for evaluating complex reasoning, paraphrasing, and domain-specific content such as medical chain-of-thought responses.\n\n---","metadata":{}},{"cell_type":"code","source":"# â”€â”€ (1) Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n!pip install -q torchmetrics transformers\n\n# â”€â”€ (2) Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfrom torchmetrics.text.bert import BERTScore\nimport torch\n\n# â”€â”€ (3) Prepare your predictions & references â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npredictions = val_df[\"finetuned_pred\"].tolist()\nreferences  = val_df[\"Response\"].tolist()\n\n# â”€â”€ (4) Initialize the class-based metric properly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nbertmetric = BERTScore(\n    model_name_or_path=\"microsoft/deberta-v3-base\",\n    lang=\"en\",\n    rescale_with_baseline=False,\n    max_length=512,         # ensures consistent length\n    truncation=True,        # enable sequence truncation\n    batch_size=8,           # adjust based on GPU memory\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    verbose=True\n)\n\n\n# â”€â”€ (5) Compute scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nresults = bertmetric(predictions, references)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:09:18.079476Z","iopub.execute_input":"2025-07-02T15:09:18.080050Z","iopub.status.idle":"2025-07-02T15:09:38.702215Z","shell.execute_reply.started":"2025-07-02T15:09:18.080029Z","shell.execute_reply":"2025-07-02T15:09:38.701579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56216e7c48184c068de9944f6c88b814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54309d5a2e1249e28a152fd76c5aa2bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7904c3815747578e76b1a26d7efccb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac65d71893cd4ee1a11c9d9935f70efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f177fa1acb49b58cf8acd101bcea9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2fabbe0f604ed5b9a1e6c0d733d9c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adbe10930a224939b20346f9b15fa980"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"\n\n# â”€â”€ (6) Display mean metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(f\"BERTScore Precision: {results['precision'].mean():.4f}\")\nprint(f\"BERTScore Recall:    {results['recall'].mean():.4f}\")\nprint(f\"BERTScore F1:        {results['f1'].mean():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:09:38.702948Z","iopub.execute_input":"2025-07-02T15:09:38.703594Z","iopub.status.idle":"2025-07-02T15:09:38.711440Z","shell.execute_reply.started":"2025-07-02T15:09:38.703571Z","shell.execute_reply":"2025-07-02T15:09:38.710661Z"}},"outputs":[{"name":"stdout","text":"BERTScore Precision: 0.7345\nBERTScore Recall:    0.8033\nBERTScore F1:        0.7670\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"\n\n#### ðŸ” Model Performance using `microsoft/deberta-v3-base`:\n\n| Metric     | Score   |\n|------------|---------|\n| ðŸ§  **Precision** | `0.7345` |\n| ðŸ§  **Recall**    | `0.8033` |\n| ðŸ§  **F1 Score**  | `0.7670` |\n\n---\n\n### ðŸ“Œ Interpretation\n\n- **Precision** (`73.4%`): Measures how much of the modelâ€™s output is semantically aligned with the reference.  \n- **Recall** (`80.33%`): Reflects how much of the reference content is captured in the modelâ€™s response.  \n- **F1 Score** (`76.70%`): Harmonizes precision and recall to give an overall semantic match quality.\n\nðŸ§ª These scores suggest that the fine-tuned **LLaMA 3.2 model** performs well on the **medical reasoning task**, generating outputs that are both contextually rich and a\n","metadata":{}},{"cell_type":"markdown","source":"## save the model","metadata":{}},{"cell_type":"code","source":"save_path = \"llama3-medical-finetuned\"\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:15:23.954264Z","iopub.execute_input":"2025-07-02T15:15:23.954758Z","iopub.status.idle":"2025-07-02T15:15:24.818064Z","shell.execute_reply.started":"2025-07-02T15:15:23.954726Z","shell.execute_reply":"2025-07-02T15:15:24.817368Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"('llama3-medical-finetuned/tokenizer_config.json',\n 'llama3-medical-finetuned/special_tokens_map.json',\n 'llama3-medical-finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:15:27.274810Z","iopub.execute_input":"2025-07-02T15:15:27.275129Z","iopub.status.idle":"2025-07-02T15:15:27.292190Z","shell.execute_reply.started":"2025-07-02T15:15:27.275106Z","shell.execute_reply":"2025-07-02T15:15:27.291201Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9252d933cfa54cecb23225f9c2942bac"}},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"model.push_to_hub(\"SaadKabeer/llama3-medical-finetuned\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:16:01.849318Z","iopub.execute_input":"2025-07-02T15:16:01.849587Z","iopub.status.idle":"2025-07-02T15:16:04.311723Z","shell.execute_reply.started":"2025-07-02T15:16:01.849566Z","shell.execute_reply":"2025-07-02T15:16:04.311006Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79707b1ba4154b20bd0b0e786a650b4a"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/SaadKabeer/llama3-medical-finetuned\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"tokenizer.push_to_hub(\"SaadKabeer/llama3-medical-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:16:07.687688Z","iopub.execute_input":"2025-07-02T15:16:07.688083Z","iopub.status.idle":"2025-07-02T15:16:08.976299Z","shell.execute_reply.started":"2025-07-02T15:16:07.688057Z","shell.execute_reply":"2025-07-02T15:16:08.975733Z"}},"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}